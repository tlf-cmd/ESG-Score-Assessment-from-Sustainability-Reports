{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.text_preprocessing.preprocess_text import preprocess_text\n",
    "from helpers.text_preprocessing.add_spelling_correction import add_spelling_correction\n",
    "from helpers.ngrams.detect_ngrams import detect_ngrams\n",
    "from helpers.ngrams.add_ngrams import add_ngrams\n",
    "from helpers.pdf_to_text.remove_unnecessary_context_from_PDF import process_pdfs_in_directory\n",
    "import pandas as pd\n",
    "\n",
    "def data_pipeline(input_path, output_path='../data/extracted_text_sustainability_reports.csv', n_processes=8):  # Added n_processes with default value of 8\n",
    "\n",
    "    # Stage 0: \n",
    "    pdf_to_csv(input_path, '../data/extracted_text_sustainability_reports.csv')\n",
    "\n",
    "    # 0.1 - Read in the CSV and create a DataFrame\n",
    "    df = pd.read_csv('../data/extracted_text_sustainability_reports.csv')\n",
    "\n",
    "    # Stage 1: Text Preprocessing\n",
    "    df = preprocess_text(df)  # Note that we're directly passing the DataFrame\n",
    "\n",
    "    # Stage 2: Adding spelling correction\n",
    "    df = add_spelling_correction(df, output_folder='../data/', n_processes=n_processes)  # Added n_processes\n",
    "\n",
    "    # Stage 3: Detect ngrams\n",
    "    df, _ = detect_ngrams(df)  # Assuming detect_ngrams returns DataFrame as first element in a tuple\n",
    "\n",
    "    # Stage 4: Adding ngrams\n",
    "    df[\"preprocessed_content\"] = df[\"preprocessed_content\"].apply(add_ngrams)\n",
    "\n",
    "    # Stage 5: Save the final dataframe to CSV\n",
    "    df.to_csv('../data/ready_to_model/ready_to_model_df.csv')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.topic_modelling.generate_dtm import create_dtm\n",
    "from helpers.topic_modelling.generate_tfidf import create_tfidf\n",
    "from helpers.merge_dataframes import merge_dataframes\n",
    "from models.LDA_optuna_tuning.tune_lda_optuna import objective\n",
    "from models.LDA_optuna_tuning.call_optuna_tune import preprocess_data\n",
    "from models.LDA_optuna_tuning.call_optuna_tune import execute_optuna_study\n",
    "from models.NMF.perform_NMF import perform_nmf\n",
    "from helpers.add_topic_to_dataframe import add_topic_to_dataframe\n",
    "from helpers.topic_modelling.get_embeddings import get_embeddings\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def topic_modelling_pipeline(df, trials):\n",
    "\n",
    "# Stage 1 - Run the Optuna study to get the best model, corpus, and dictionary\n",
    "    best_model, corpus, dictionary = execute_optuna_study(df, n_trials=trials)\n",
    "\n",
    "# Stage 2 - Add the topics to the DataFrame\n",
    "    df_with_topics = add_topic_to_dataframe(df, best_model, corpus)\n",
    "\n",
    "# Stage 3 - run TFIDF and add vectors to dataframe\n",
    "\n",
    "    tfidf_matrix = create_tfidf(df_with_topics)\n",
    "    df_with_topics = pd.concat([df_with_topics, tfidf_matrix], axis=1)\n",
    "\n",
    "# Stage 4 Dimensionality reduction (Truncated SVD) for TFIDF vectors\n",
    "\n",
    "    n_components_tfidf = 200  # You can adjust this based on your needs\n",
    "    svd_tfidf = TruncatedSVD(n_components=n_components_tfidf)\n",
    "    reduced_tfidf = svd_tfidf.fit_transform(tfidf_matrix)\n",
    "    reduced_tfidf_df = pd.DataFrame(reduced_tfidf, columns=[f\"tfidf_svd_dim_{i}\" for i in range(n_components_tfidf)], index=df_with_topics.index)\n",
    "    df_with_topics = pd.concat([df_with_topics, reduced_tfidf_df], axis=1)\n",
    "    df_with_topics.drop(columns=tfidf_matrix.columns, inplace=True)\n",
    "\n",
    "# Stage 5 - Word embedding with ESG-BERT\n",
    "\n",
    "    df_with_topics['esg_bert_embeddings'] = df_with_topics['preprocessed_content'].apply(get_embeddings)\n",
    "    bert_embeddings = pd.DataFrame(df_with_topics['esg_bert_embeddings'].tolist(), index=df_with_topics.index)\n",
    "    df_with_topics = pd.concat([df_with_topics, bert_embeddings], axis=1)\n",
    "    df_with_topics.drop(columns=['esg_bert_embeddings'], inplace=True)  # Drop the original embeddings column\n",
    "\n",
    "# Stage 6 - Dimensionality reduction (Truncated SVD) for BERT embeddings\n",
    "\n",
    "    # Number of components to r etain\n",
    "    n_components = 200  # You can adjust this based on your needs\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    reduced_embeddings = svd.fit_transform(bert_embeddings)\n",
    "    reduced_embeddings_df = pd.DataFrame(reduced_embeddings, columns=[f\"svd_dim_{i}\" for i in range(n_components)], index=df_with_topics.index)\n",
    "    df_with_topics = pd.concat([df_with_topics, reduced_embeddings_df], axis=1)\n",
    "    df_with_topics = df_with_topics[df_with_topics.columns[~df_with_topics.columns.str.isnumeric()]]  # Drop the original embeddings column\n",
    "\n",
    "# Stage 7 Save the df_with_topics dataframe to CSV\n",
    "    df_with_topics.to_csv('../data/ready_to_model/df_with_topics.csv')\n",
    "\n",
    "# Stage 8 - merge all results into one dataframe\n",
    "\n",
    "    df = pd.read_csv('../data/ready_to_model/df_with_topics.csv', index_col=0) # convert df_with_topics to df\n",
    "    ESG_SP500 = pd.read_csv('../data/SP500_ESG_Score_average_per_year.csv', index_col=0) # convert ESG_SP500 to df\n",
    "    merge_dataframes(df,ESG_SP500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_directory = 'ESG report.pdf'\n",
    "output_csv = 'extracted_text_sustainability_reports.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data_pipeline('ESG report.pdf')\n",
    "topic_modelling_pipeline(df,100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
